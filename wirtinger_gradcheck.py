"""
ä¿å­˜ä½ç½®: Anla/wirtinger_gradcheck.py

Wirtinger æœ‰é™å·®åˆ†æ¢¯åº¦éªŒè¯å™¨
=========================================================================

ç›®çš„:
    å¯¹ Anla æ¯ä¸€å±‚çš„ manual_backward å®ç°è¿›è¡Œæ•°å€¼éªŒè¯ã€‚
    ç”¨æœ‰é™å·®åˆ†é€¼è¿‘çœŸå®çš„ Wirtinger æ¢¯åº¦ dL/dz*ï¼Œä¸ manual_backward
    è¿”å›çš„ grad_input åšå¯¹æ¯”ã€‚

æ ¸å¿ƒå…¬å¼:
    å®šä¹‰æ ‡é‡æŸå¤± L(z) = Re( sum( conj(G) Â· f(z) ) )
    å…¶ä¸­ G æ˜¯éšæœºä¸Šæ¸¸æ¢¯åº¦ï¼Œf æ˜¯å±‚çš„å‰å‘å‡½æ•°ã€‚

    åˆ™ dL/dz_k* = (1/2) * [
        (L(z + ÎµÂ·e_k) - L(z - ÎµÂ·e_k)) / (2Îµ)
      + iÂ·(L(z + iÎµÂ·e_k) - L(z - iÎµÂ·e_k)) / (2Îµ)
    ]

    è¿™åº”è¯¥ç­‰äº manual_backward(G) åœ¨ç¬¬ k ä¸ªå…ƒç´ çš„å€¼ã€‚

æ£€æµ‹å±‚ (æŒ‰å«Œç–‘æ’åº):
    1. MagPhaseSoftmax â€” å¤´å·å«Œç–‘, éä¸¥æ ¼æ¨å¯¼
    2. HolographicAttention â€” å®Œæ•´æ³¨æ„åŠ›é“¾è·¯
    3. PhaseTwist â€” æ–°ç‰ˆåŒå‘è€¦åˆ
    4. ComplexRMSNorm â€” åˆšä¿®æ­£ç¬¦å·
    5. ComplexLinear â€” åŸºç¡€ç»„ä»¶
    6. ComplexRotaryEmbedding â€” åº”è¯¥é€æ˜

ç”¨æ³•:
    python -m Anla.wirtinger_gradcheck
    python -m Anla.wirtinger_gradcheck --layer MagPhaseSoftmax
    python -m Anla.wirtinger_gradcheck --layer all --eps 1e-5 --n-samples 20
"""

import argparse
import copy
import sys
import os
import time
from typing import Callable, Dict, List, Optional, Tuple, Any
from collections import OrderedDict

import torch
import torch.nn as nn

# ==========================================================================
#  æ ¸å¿ƒ: Wirtinger æœ‰é™å·®åˆ†
# ==========================================================================

def wirtinger_fd_gradient(
    forward_fn: Callable[[torch.Tensor], torch.Tensor],
    z: torch.Tensor,
    upstream_grad: torch.Tensor,
    eps: float = 1e-5,
    sample_indices: Optional[List[Tuple[int, ...]]] = None,
) -> torch.Tensor:
    """
    ç”¨æœ‰é™å·®åˆ†è®¡ç®— dL/dz*ï¼Œå…¶ä¸­ L(z) = Re(sum(conj(G) * f(z)))ã€‚

    Parameters
    ----------
    forward_fn : callable
        z -> f(z)ï¼Œçº¯å‡½æ•°ï¼ˆæ— å‰¯ä½œç”¨ï¼‰
    z : torch.Tensor (complex)
        è¾“å…¥å¼ é‡
    upstream_grad : torch.Tensor (complex)
        ä¸Šæ¸¸æ¢¯åº¦ G = dL/df*
    eps : float
        æœ‰é™å·®åˆ†æ­¥é•¿
    sample_indices : list of tuples, optional
        åªåœ¨è¿™äº›ä½ç½®è®¡ç®—æœ‰é™å·®åˆ†ï¼ˆåŠ é€Ÿå¤§å¼ é‡ï¼‰
        å¦‚æœ Noneï¼Œè®¡ç®—æ‰€æœ‰ä½ç½®

    Returns
    -------
    grad_fd : torch.Tensor (complex)
        æœ‰é™å·®åˆ†é€¼è¿‘çš„ dL/dz*
    """
    z_flat = z.detach().clone().reshape(-1)
    n = z_flat.shape[0]

    # å¦‚æœæ²¡æœ‰æŒ‡å®šé‡‡æ ·ä½ç½®ï¼Œå…¨éƒ¨è®¡ç®—
    if sample_indices is None:
        indices = list(range(n))
    else:
        indices = [_multi_to_flat(idx, z.shape) for idx in sample_indices]

    grad_fd_flat = torch.zeros(n, dtype=z.dtype, device=z.device)

    G = upstream_grad.detach()

    def scalar_loss(z_perturbed_flat):
        z_p = z_perturbed_flat.reshape(z.shape)
        f_val = forward_fn(z_p)
        # L = 2 * Re(sum(conj(G) * f))
        # å› å­ 2 ä½¿å¾— dL/dz* ä¸ Wirtinger é“¾å¼æ³•åˆ™çº¦å®šä¸€è‡´:
        #   dL/dz* = conj(G)Â·(df/dz*) + GÂ·conj(df/dz)
        # ä¸å« 2 æ—¶, Re(sum(conj(G)*f)) çš„ Wirtinger å¯¼æ•°åªæœ‰ä¸Šå¼çš„ 1/2
        return 2.0 * torch.real(torch.sum(torch.conj(G) * f_val)).item()

    for k in indices:
        # å®æ–¹å‘æ‰°åŠ¨: (L(z+Îµe_k) - L(z-Îµe_k)) / (2Îµ)
        z_plus = z_flat.clone()
        z_plus[k] = z_plus[k] + eps
        z_minus = z_flat.clone()
        z_minus[k] = z_minus[k] - eps
        dL_real = (scalar_loss(z_plus) - scalar_loss(z_minus)) / (2.0 * eps)

        # è™šæ–¹å‘æ‰°åŠ¨: (L(z+iÎµe_k) - L(z-iÎµe_k)) / (2Îµ)
        z_plus_i = z_flat.clone()
        z_plus_i[k] = z_plus_i[k] + 1j * eps
        z_minus_i = z_flat.clone()
        z_minus_i[k] = z_minus_i[k] - 1j * eps
        dL_imag = (scalar_loss(z_plus_i) - scalar_loss(z_minus_i)) / (2.0 * eps)

        # dL/dz* = (1/2)(dL_real + i * dL_imag)
        grad_fd_flat[k] = 0.5 * (dL_real + 1j * dL_imag)

    return grad_fd_flat.reshape(z.shape)


def _multi_to_flat(multi_idx, shape):
    """å¤šç»´ç´¢å¼•è½¬ä¸€ç»´ç´¢å¼•"""
    flat = 0
    for i, s in zip(multi_idx, shape):
        flat = flat * s + i
    return flat


def random_sample_indices(shape, n_samples=20, rng=None):
    """éšæœºé€‰å– n_samples ä¸ªä½ç½®"""
    if rng is None:
        rng = torch.Generator()
    total = 1
    for s in shape:
        total *= s
    n_samples = min(n_samples, total)
    flat_indices = torch.randperm(total, generator=rng)[:n_samples]
    multi_indices = []
    for fi in flat_indices:
        fi = fi.item()
        idx = []
        for s in reversed(shape):
            idx.append(fi % s)
            fi //= s
        idx.reverse()
        multi_indices.append(tuple(idx))
    return multi_indices


# ==========================================================================
#  æ¯”è¾ƒæŒ‡æ ‡
# ==========================================================================

def compare_gradients(
    grad_manual: torch.Tensor,
    grad_fd: torch.Tensor,
    sample_indices: Optional[List[Tuple[int, ...]]] = None,
) -> Dict[str, float]:
    """æ¯”è¾ƒæ‰‹åŠ¨æ¢¯åº¦ä¸æœ‰é™å·®åˆ†æ¢¯åº¦"""
    if sample_indices is not None:
        # åªæ¯”è¾ƒé‡‡æ ·ä½ç½®
        m_vals = torch.stack([grad_manual[idx] for idx in sample_indices])
        f_vals = torch.stack([grad_fd[idx] for idx in sample_indices])
    else:
        m_vals = grad_manual.reshape(-1)
        f_vals = grad_fd.reshape(-1)

    # ä½™å¼¦ç›¸ä¼¼åº¦ (æŠŠå¤æ•°å±•å¼€ä¸ºå®è™šæ‹¼æ¥çš„å®å‘é‡)
    m_real = torch.cat([m_vals.real, m_vals.imag])
    f_real = torch.cat([f_vals.real, f_vals.imag])

    dot = torch.dot(m_real, f_real)
    norm_m = m_real.norm()
    norm_f = f_real.norm()
    cosine = (dot / (norm_m * norm_f + 1e-30)).item()

    # ç›¸å¯¹è¯¯å·®
    diff = (m_vals - f_vals).abs()
    rel_err = (diff / (f_vals.abs() + 1e-30)).mean().item()
    max_rel_err = (diff / (f_vals.abs() + 1e-30)).max().item()

    # ç»å¯¹è¯¯å·®
    abs_err = diff.mean().item()
    max_abs_err = diff.max().item()

    # å¹…å€¼æ¯”
    mag_ratio = (norm_m / (norm_f + 1e-30)).item()

    return {
        "cosine_similarity": cosine,
        "mean_relative_error": rel_err,
        "max_relative_error": max_rel_err,
        "mean_absolute_error": abs_err,
        "max_absolute_error": max_abs_err,
        "magnitude_ratio": mag_ratio,
        "manual_norm": norm_m.item(),
        "fd_norm": norm_f.item(),
        "n_compared": len(m_vals),
    }


# ==========================================================================
#  å‚æ•°ä¿å­˜/æ¢å¤ (å¤„ç† manual_backward çš„å‰¯ä½œç”¨)
# ==========================================================================

def save_layer_state(layer: nn.Module) -> Dict[str, torch.Tensor]:
    """ä¿å­˜å±‚çš„å…¨éƒ¨å‚æ•°å’Œ buffer çŠ¶æ€"""
    state = {}
    for name, param in layer.named_parameters():
        state[f"param_{name}"] = param.data.clone()
    for name, buf in layer.named_buffers():
        state[f"buffer_{name}"] = buf.clone()
    return state


def restore_layer_state(layer: nn.Module, state: Dict[str, torch.Tensor]):
    """æ¢å¤å±‚çš„çŠ¶æ€"""
    for name, param in layer.named_parameters():
        key = f"param_{name}"
        if key in state:
            param.data.copy_(state[key])
    for name, buf in layer.named_buffers():
        key = f"buffer_{name}"
        if key in state:
            buf.copy_(state[key])


# ==========================================================================
#  é€å±‚æµ‹è¯•å‡½æ•°
# ==========================================================================

def test_mag_phase_softmax(eps=1e-5, n_samples=30, seed=42):
    """
    æµ‹è¯• MagPhaseSoftmax çš„ manual_backwardã€‚
    å¤´å·å«Œç–‘ï¼šéä¸¥æ ¼æ¨å¯¼çš„è¿‘ä¼¼åä¼ ã€‚
    """
    from Anla.layers.holographic_attention import MagPhaseSoftmax

    torch.manual_seed(seed)
    layer = MagPhaseSoftmax(dim=-1)
    layer.train()

    # è¾“å…¥: æ³¨æ„åŠ›åˆ†æ•°çŸ©é˜µ (Batch=2, Heads=2, Seq_Q=4, Seq_K=4)
    B, H, SQ, SK = 2, 2, 4, 4
    scale = 1.0 / (16 ** 0.5)  # æ¨¡æ‹Ÿ 1/sqrt(head_dim)

    z = torch.randn(B, H, SQ, SK, dtype=torch.cfloat) * 0.5 + 0.3
    G = torch.randn(B, H, SQ, SK, dtype=torch.cfloat) * 0.1

    # --- æ‰‹åŠ¨åä¼  ---
    state = save_layer_state(layer)
    layer.forward(z, scale=scale)
    grad_manual = layer.manual_backward(G, learning_rate=0.001)
    restore_layer_state(layer, state)

    # --- æœ‰é™å·®åˆ† ---
    sample_idx = random_sample_indices(z.shape, n_samples, torch.Generator().manual_seed(seed))

    def fwd_fn(z_in):
        # éœ€è¦é‡æ–°åˆ›å»ºå±‚å®ä¾‹ä»¥é¿å…ç¼“å­˜æ±¡æŸ“
        layer_clean = MagPhaseSoftmax(dim=-1)
        layer_clean.eval()
        return layer_clean.forward(z_in, scale=scale)

    grad_fd = wirtinger_fd_gradient(fwd_fn, z, G, eps=eps, sample_indices=sample_idx)

    return compare_gradients(grad_manual, grad_fd, sample_idx)


def test_phase_twist(eps=1e-5, n_samples=30, seed=42):
    """
    æµ‹è¯• PhaseTwist (åŒå‘è€¦åˆæ¿€æ´»å‡½æ•°) çš„ manual_backwardã€‚
    """
    from Anla.layers.activation import PhaseTwist

    torch.manual_seed(seed)
    channels = 16
    layer = PhaseTwist(channels, init_gamma=0.05, init_beta=0.05, init_phi=0.1)
    layer.train()

    B, S = 2, 8
    z = torch.randn(B, S, channels, dtype=torch.cfloat) * 0.5 + 0.3
    G = torch.randn(B, S, channels, dtype=torch.cfloat) * 0.1

    # --- æ‰‹åŠ¨åä¼  ---
    state = save_layer_state(layer)
    layer.forward(z)
    grad_manual = layer.manual_backward(G, learning_rate=0.001)
    restore_layer_state(layer, state)

    # --- æœ‰é™å·®åˆ† ---
    sample_idx = random_sample_indices(z.shape, n_samples, torch.Generator().manual_seed(seed))

    # å›ºå®šå‚æ•°å¿«ç…§
    gamma_snap = layer.gamma.data.clone()
    beta_snap = layer.beta.data.clone()
    phi_snap = layer.phi.data.clone()

    def fwd_fn(z_in):
        layer_eval = PhaseTwist(channels)
        layer_eval.gamma.data.copy_(gamma_snap)
        layer_eval.beta.data.copy_(beta_snap)
        layer_eval.phi.data.copy_(phi_snap)
        layer_eval.eval()
        return layer_eval.forward(z_in)

    grad_fd = wirtinger_fd_gradient(fwd_fn, z, G, eps=eps, sample_indices=sample_idx)

    return compare_gradients(grad_manual, grad_fd, sample_idx)


def test_complex_rms_norm(eps=1e-5, n_samples=30, seed=42):
    """
    æµ‹è¯• ComplexRMSNorm çš„ manual_backwardã€‚
    """
    from Anla.layers.normalization import ComplexRMSNorm

    torch.manual_seed(seed)
    dim = 16
    layer = ComplexRMSNorm(dim)
    layer.train()

    B, S = 2, 8
    z = torch.randn(B, S, dim, dtype=torch.cfloat) * 0.5 + 0.3
    G = torch.randn(B, S, dim, dtype=torch.cfloat) * 0.1

    state = save_layer_state(layer)
    layer.forward(z)
    grad_manual = layer.manual_backward(G, learning_rate=0.001)
    restore_layer_state(layer, state)

    sample_idx = random_sample_indices(z.shape, n_samples, torch.Generator().manual_seed(seed))

    scale_snap = layer.scale.data.clone()

    def fwd_fn(z_in):
        lyr = ComplexRMSNorm(dim)
        lyr.scale.data.copy_(scale_snap)
        lyr.eval()
        return lyr.forward(z_in)

    grad_fd = wirtinger_fd_gradient(fwd_fn, z, G, eps=eps, sample_indices=sample_idx)

    return compare_gradients(grad_manual, grad_fd, sample_idx)


def test_complex_linear(eps=1e-5, n_samples=30, seed=42):
    """
    æµ‹è¯• ComplexLinear çš„ manual_backward (è¾“å…¥æ¢¯åº¦éƒ¨åˆ†)ã€‚
    """
    from Anla.layers.linear import ComplexLinear

    torch.manual_seed(seed)
    in_feat, out_feat = 16, 16
    layer = ComplexLinear(in_feat, out_feat, bias=True, mode='descent')
    layer.train()

    B, S = 2, 8
    z = torch.randn(B, S, in_feat, dtype=torch.cfloat) * 0.3
    G = torch.randn(B, S, out_feat, dtype=torch.cfloat) * 0.1

    state = save_layer_state(layer)
    layer.forward(z)
    grad_manual = layer.manual_backward(G, learning_rate=0.001, weight_decay=0.0)
    restore_layer_state(layer, state)

    sample_idx = random_sample_indices(z.shape, n_samples, torch.Generator().manual_seed(seed))

    w_snap = layer.weight.data.clone()
    b_snap = layer.bias.data.clone() if layer.bias is not None else None

    def fwd_fn(z_in):
        lyr = ComplexLinear(in_feat, out_feat, bias=True, mode='descent')
        lyr.weight.data.copy_(w_snap)
        if b_snap is not None:
            lyr.bias.data.copy_(b_snap)
        lyr.eval()
        return lyr.forward(z_in)

    grad_fd = wirtinger_fd_gradient(fwd_fn, z, G, eps=eps, sample_indices=sample_idx)

    return compare_gradients(grad_manual, grad_fd, sample_idx)


def test_complex_rotary(eps=1e-5, n_samples=30, seed=42):
    """
    æµ‹è¯• ComplexRotaryEmbedding çš„ manual_backwardã€‚
    """
    from Anla.layers.positional import ComplexRotaryEmbedding

    torch.manual_seed(seed)
    dim = 16
    layer = ComplexRotaryEmbedding(dim, max_seq_len=64)

    B, S = 2, 8
    z = torch.randn(B, S, dim, dtype=torch.cfloat) * 0.5
    G = torch.randn(B, S, dim, dtype=torch.cfloat) * 0.1

    layer.forward(z)  # rotary has no training state to worry about
    grad_manual = layer.manual_backward(G)

    sample_idx = random_sample_indices(z.shape, n_samples, torch.Generator().manual_seed(seed))

    def fwd_fn(z_in):
        lyr = ComplexRotaryEmbedding(dim, max_seq_len=64)
        lyr.rotary_emb = layer.rotary_emb.clone()
        return lyr.forward(z_in)

    grad_fd = wirtinger_fd_gradient(fwd_fn, z, G, eps=eps, sample_indices=sample_idx)

    return compare_gradients(grad_manual, grad_fd, sample_idx)


def test_holographic_attention(eps=1e-5, n_samples=20, seed=42):
    """
    æµ‹è¯• HolographicAttention çš„ç«¯åˆ°ç«¯ manual_backwardã€‚
    æ³¨æ„: è¿™åŒ…å«äº† MagPhaseSoftmax + 4 ä¸ª ComplexLinear çš„ç»„åˆã€‚
    """
    from Anla.layers.holographic_attention import HolographicAttention

    torch.manual_seed(seed)
    d_model = 16
    num_heads = 2
    layer = HolographicAttention(d_model, num_heads=num_heads)
    layer.train()

    B, S = 2, 6
    z = torch.randn(B, S, d_model, dtype=torch.cfloat) * 0.3
    G = torch.randn(B, S, d_model, dtype=torch.cfloat) * 0.1

    state = save_layer_state(layer)
    layer.forward(z)
    grad_manual = layer.manual_backward(G, learning_rate=0.001, weight_decay=0.0)
    restore_layer_state(layer, state)

    sample_idx = random_sample_indices(z.shape, n_samples, torch.Generator().manual_seed(seed))

    # éœ€è¦æ·±æ‹·è´æ•´ä¸ªå±‚ï¼ˆå«æ‰€æœ‰å­å±‚æƒé‡ï¼‰
    def make_clean_layer():
        lyr = HolographicAttention(d_model, num_heads=num_heads)
        lyr.load_state_dict(
            {k: v.clone() for k, v in
             zip(layer.state_dict().keys(),
                 [state.get(f"param_{k}", state.get(f"buffer_{k}", v))
                  for k, v in layer.state_dict().items()])}
        )
        # ç›´æ¥ç”¨ä¿å­˜çš„ state_dict æ›´ç®€æ´
        return lyr

    # ä¿å­˜å®Œæ•´ state_dict
    full_sd = {k: v.clone() for k, v in layer.state_dict().items()}
    # æ¢å¤åˆ°æ£€æŸ¥ç‚¹çŠ¶æ€
    restore_layer_state(layer, state)

    def fwd_fn(z_in):
        lyr = HolographicAttention(d_model, num_heads=num_heads)
        lyr.load_state_dict({k: v.clone() for k, v in full_sd.items()})
        lyr.eval()
        return lyr.forward(z_in)

    grad_fd = wirtinger_fd_gradient(fwd_fn, z, G, eps=eps, sample_indices=sample_idx)

    return compare_gradients(grad_manual, grad_fd, sample_idx)


def test_transformer_block(eps=1e-5, n_samples=15, seed=42):
    """
    æµ‹è¯• ComplexTransformerBlock çš„ç«¯åˆ°ç«¯ manual_backwardã€‚
    æœ€å…¨é¢çš„æµ‹è¯•ï¼šåŒ…å«æ³¨æ„åŠ› + FFN + Norm + Residualã€‚
    """
    from Anla.layers.transformer_block import ComplexTransformerBlock

    torch.manual_seed(seed)
    d_model = 16
    num_heads = 2
    layer = ComplexTransformerBlock(d_model, num_heads=num_heads, ff_mult=2)
    layer.train()

    B, S = 2, 6
    z = torch.randn(B, S, d_model, dtype=torch.cfloat) * 0.3
    G = torch.randn(B, S, d_model, dtype=torch.cfloat) * 0.1

    state = save_layer_state(layer)
    layer.forward(z)
    grad_manual = layer.manual_backward(G, lr=0.001, wd=0.0)
    restore_layer_state(layer, state)

    sample_idx = random_sample_indices(z.shape, n_samples, torch.Generator().manual_seed(seed))

    full_sd = {k: v.clone() for k, v in layer.state_dict().items()}
    restore_layer_state(layer, state)

    def fwd_fn(z_in):
        lyr = ComplexTransformerBlock(d_model, num_heads=num_heads, ff_mult=2)
        lyr.load_state_dict({k: v.clone() for k, v in full_sd.items()})
        lyr.eval()
        return lyr.forward(z_in)

    grad_fd = wirtinger_fd_gradient(fwd_fn, z, G, eps=eps, sample_indices=sample_idx)

    return compare_gradients(grad_manual, grad_fd, sample_idx)


# ==========================================================================
#  å‚æ•°æ¢¯åº¦éªŒè¯ (é¢å¤–: æ£€æŸ¥ dL/d(param) æ˜¯å¦æ­£ç¡®)
# ==========================================================================

def test_param_gradients_phase_twist(eps=1e-5, n_samples=10, seed=42):
    """
    éªŒè¯ PhaseTwist çš„å‚æ•°æ¢¯åº¦ (gamma, beta, phi)ã€‚
    é€šè¿‡æ¯”è¾ƒå‚æ•°æ›´æ–°å‰åçš„å˜åŒ–é‡ä¸æœ‰é™å·®åˆ†ã€‚
    """
    from Anla.layers.activation import PhaseTwist

    torch.manual_seed(seed)
    channels = 8
    layer = PhaseTwist(channels, init_gamma=0.05, init_beta=0.05, init_phi=0.1)
    layer.train()

    B, S = 2, 4
    z = torch.randn(B, S, channels, dtype=torch.cfloat) * 0.5 + 0.3
    G = torch.randn(B, S, channels, dtype=torch.cfloat) * 0.1

    results = {}

    for param_name in ['gamma', 'beta', 'phi']:
        param = getattr(layer, param_name)
        param_snap = param.data.clone()

        # æœ‰é™å·®åˆ†: å¯¹å‚æ•°çš„æ¯ä¸ªå…ƒç´ æ‰°åŠ¨
        grad_fd = torch.zeros_like(param)

        for k in range(min(n_samples, param.numel())):
            def scalar_loss_for_param(p_val):
                p_clone = param_snap.clone()
                p_clone[k] = p_val
                # åˆ›å»ºå¹²å‡€å±‚
                lyr = PhaseTwist(channels)
                lyr.gamma.data.copy_(layer.gamma.data.clone() if param_name != 'gamma' else p_clone)
                lyr.beta.data.copy_(layer.beta.data.clone() if param_name != 'beta' else p_clone)
                lyr.phi.data.copy_(layer.phi.data.clone() if param_name != 'phi' else p_clone)
                lyr.eval()
                f_val = lyr.forward(z)
                return torch.real(torch.sum(torch.conj(G) * f_val)).item()

            # å®å‚æ•°åªéœ€è¦å®æ–¹å‘å·®åˆ†
            L_plus = scalar_loss_for_param(param_snap[k] + eps)
            L_minus = scalar_loss_for_param(param_snap[k] - eps)
            grad_fd[k] = (L_plus - L_minus) / (2.0 * eps)

        # æ‰‹åŠ¨åä¼ å¾—åˆ°çš„å‚æ•°æ¢¯åº¦ (é€šè¿‡è§‚å¯Ÿå‚æ•°å˜åŒ–é‡)
        state = save_layer_state(layer)
        lr_test = 1.0  # ç”¨ lr=1 ä½¿å¾— param_new = param_old - grad
        layer.forward(z)
        layer.manual_backward(G, learning_rate=lr_test)
        param_after = getattr(layer, param_name).data.clone()
        restore_layer_state(layer, state)

        # param_new = param_old - lr * grad  =>  grad = (param_old - param_new) / lr
        grad_manual_param = (param_snap - param_after) / lr_test

        # æ¯”è¾ƒ
        n_cmp = min(n_samples, param.numel())
        m_vals = grad_manual_param[:n_cmp]
        f_vals = grad_fd[:n_cmp]

        cos_sim = torch.dot(m_vals, f_vals) / (m_vals.norm() * f_vals.norm() + 1e-30)
        rel_err = ((m_vals - f_vals).abs() / (f_vals.abs() + 1e-30)).mean()

        results[param_name] = {
            "cosine_similarity": cos_sim.item(),
            "mean_relative_error": rel_err.item(),
            "manual_norm": m_vals.norm().item(),
            "fd_norm": f_vals.norm().item(),
        }

    return results


# ==========================================================================
#  Softmax Jacobian å•ç‹¬éªŒè¯
# ==========================================================================

def test_softmax_jacobian_only(eps=1e-6, n_samples=20, seed=42):
    """
    å•ç‹¬éªŒè¯: softmax(|z|*scale) å¯¹ |z| çš„åä¼ æ˜¯å¦æ­£ç¡®ã€‚
    å‰¥ç¦»ç›¸ä½éƒ¨åˆ†ï¼Œåªçœ‹ MagPhaseSoftmax çš„æ¨¡é•¿é€šè·¯ã€‚
    """
    torch.manual_seed(seed)
    B, H, SQ, SK = 1, 1, 4, 4
    scale = 0.25

    mag = torch.rand(B, H, SQ, SK) * 2.0 + 0.1  # æ­£å®æ•°

    # forward: y = softmax(mag * scale)
    def fwd_mag(m):
        return torch.softmax(m * scale, dim=-1)

    y = fwd_mag(mag)
    G_real = torch.randn_like(mag) * 0.1

    # æ‰‹åŠ¨ softmax backward
    tmp = y * G_real
    sum_tmp = tmp.sum(dim=-1, keepdim=True)
    grad_manual_mag = (tmp - y * sum_tmp) * scale

    # æœ‰é™å·®åˆ†
    grad_fd_mag = torch.zeros_like(mag)
    flat_mag = mag.reshape(-1)
    n = flat_mag.shape[0]
    indices = list(range(min(n_samples, n)))

    for k in indices:
        m_plus = flat_mag.clone()
        m_plus[k] += eps
        m_minus = flat_mag.clone()
        m_minus[k] -= eps

        L_plus = torch.sum(G_real * fwd_mag(m_plus.reshape(mag.shape))).item()
        L_minus = torch.sum(G_real * fwd_mag(m_minus.reshape(mag.shape))).item()
        grad_fd_mag.reshape(-1)[k] = (L_plus - L_minus) / (2.0 * eps)

    # æ¯”è¾ƒ
    m_v = grad_manual_mag.reshape(-1)[:len(indices)]
    f_v = grad_fd_mag.reshape(-1)[:len(indices)]
    cos = torch.dot(m_v, f_v) / (m_v.norm() * f_v.norm() + 1e-30)

    return {
        "softmax_jacobian_cosine": cos.item(),
        "softmax_jacobian_rel_err": ((m_v - f_v).abs() / (f_v.abs() + 1e-30)).mean().item(),
    }


# ==========================================================================
#  ä¸»æŠ¥å‘Š
# ==========================================================================

def format_result(name: str, result: Dict[str, Any], threshold: float = 0.95):
    """æ ¼å¼åŒ–å•ä¸ªæµ‹è¯•ç»“æœ"""
    cos = result.get("cosine_similarity", -999)
    rel = result.get("mean_relative_error", 999)

    if cos > threshold:
        verdict = "âœ… PASS"
    elif cos > 0.8:
        verdict = "âš ï¸  WARN"
    elif cos > 0.5:
        verdict = "âŒ FAIL"
    else:
        verdict = "ğŸ’€ CRITICAL"

    lines = [
        f"\n{'='*70}",
        f"  {name}",
        f"{'='*70}",
        f"  Verdict:              {verdict}",
        f"  Cosine Similarity:    {cos:.6f}",
        f"  Mean Relative Error:  {rel:.6f}",
    ]

    if "max_relative_error" in result:
        lines.append(f"  Max Relative Error:   {result['max_relative_error']:.6f}")
    if "magnitude_ratio" in result:
        lines.append(f"  Magnitude Ratio:      {result['magnitude_ratio']:.6f}")
    if "manual_norm" in result:
        lines.append(f"  Manual Grad Norm:     {result['manual_norm']:.6e}")
    if "fd_norm" in result:
        lines.append(f"  FD Grad Norm:         {result['fd_norm']:.6e}")
    if "n_compared" in result:
        lines.append(f"  Points Compared:      {result['n_compared']}")

    return "\n".join(lines)


def run_all_tests(eps=1e-5, n_samples=20, seed=42, layers=None):
    """è¿è¡Œæ‰€æœ‰æ¢¯åº¦æ£€æŸ¥"""

    all_tests = OrderedDict([
        ("1. MagPhaseSoftmax (HEAD SUSPECT)", test_mag_phase_softmax),
        ("2. HolographicAttention (Full)", test_holographic_attention),
        ("3. PhaseTwist (Bidirectional)", test_phase_twist),
        ("4. ComplexRMSNorm (Corrected)", test_complex_rms_norm),
        ("5. ComplexLinear", test_complex_linear),
        ("6. ComplexRotaryEmbedding", test_complex_rotary),
        ("7. ComplexTransformerBlock (E2E)", test_transformer_block),
    ])

    auxiliary_tests = OrderedDict([
        ("AUX: Softmax Jacobian Isolation", test_softmax_jacobian_only),
        ("AUX: PhaseTwist Param Gradients", test_param_gradients_phase_twist),
    ])

    if layers is not None and layers != ['all']:
        # æŒ‰åç§°è¿‡æ»¤
        filtered = OrderedDict()
        for k, v in list(all_tests.items()) + list(auxiliary_tests.items()):
            for l in layers:
                if l.lower() in k.lower():
                    filtered[k] = v
                    break
        all_tests = filtered
        auxiliary_tests = OrderedDict()

    print("=" * 70)
    print("  Anla Wirtinger Gradient Checker")
    print("  æœ‰é™å·®åˆ† vs manual_backward æ•°å€¼å¯¹æ¯”")
    print("=" * 70)
    print(f"  eps = {eps}")
    print(f"  n_samples = {n_samples}")
    print(f"  seed = {seed}")
    print(f"  dtype = torch.cfloat (complex64)")
    print()

    results = {}
    summary = []

    for name, test_fn in all_tests.items():
        print(f"\n  Running: {name} ...", end=" ", flush=True)
        t0 = time.time()
        try:
            result = test_fn(eps=eps, n_samples=n_samples, seed=seed)
            dt = time.time() - t0
            print(f"({dt:.1f}s)")
            print(format_result(name, result))
            results[name] = result
            cos = result.get("cosine_similarity", -999)
            summary.append((name, cos))
        except Exception as e:
            dt = time.time() - t0
            print(f"ERROR ({dt:.1f}s)")
            print(f"  Exception: {e}")
            import traceback
            traceback.print_exc()
            summary.append((name, -999))

    for name, test_fn in auxiliary_tests.items():
        print(f"\n  Running: {name} ...", end=" ", flush=True)
        t0 = time.time()
        try:
            result = test_fn(eps=eps, n_samples=n_samples, seed=seed)
            dt = time.time() - t0
            print(f"({dt:.1f}s)")

            if isinstance(result, dict) and all(isinstance(v, dict) for v in result.values()):
                # åµŒå¥—ç»“æœ (å¦‚ param gradients)
                for sub_name, sub_result in result.items():
                    full_name = f"{name} â†’ {sub_name}"
                    print(format_result(full_name, sub_result))
                    cos = sub_result.get("cosine_similarity", -999)
                    summary.append((full_name, cos))
            else:
                print(format_result(name, result))
                cos = result.get("cosine_similarity",
                                 result.get("softmax_jacobian_cosine", -999))
                summary.append((name, cos))
        except Exception as e:
            dt = time.time() - t0
            print(f"ERROR ({dt:.1f}s)")
            print(f"  Exception: {e}")
            import traceback
            traceback.print_exc()

    # ========== æ€»ç»“ ==========
    print("\n")
    print("=" * 70)
    print("  SUMMARY")
    print("=" * 70)
    print(f"  {'Layer':<50} {'Cosine':>8}  {'Status':>10}")
    print(f"  {'-'*50} {'-'*8}  {'-'*10}")

    n_pass = 0
    n_fail = 0
    for name, cos in summary:
        if cos > 0.95:
            status = "âœ… PASS"
            n_pass += 1
        elif cos > 0.8:
            status = "âš ï¸  WARN"
            n_fail += 1
        elif cos > -900:
            status = "âŒ FAIL"
            n_fail += 1
        else:
            status = "ğŸ’¥ ERROR"
            n_fail += 1
        print(f"  {name:<50} {cos:>8.4f}  {status:>10}")

    print()
    print(f"  Total: {n_pass} passed, {n_fail} failed/warned")
    print()

    if n_fail > 0:
        print("  âš ï¸  DIAGNOSIS: å­˜åœ¨æ¢¯åº¦ä¸ä¸€è‡´ã€‚")
        print("  cosine < 0.95 çš„å±‚çš„ manual_backward å®ç°æœ‰æ•°å­¦é”™è¯¯ã€‚")
        print("  å»ºè®®: å¯¹ FAIL/WARN å±‚é‡æ–°æ¨å¯¼ Wirtinger å¯¼æ•°ã€‚")
    else:
        print("  âœ… æ‰€æœ‰å±‚é€šè¿‡æ¢¯åº¦éªŒè¯ã€‚")
        print("  å¦‚æœæ‹“æ‰‘ä»æœªæ¶Œç°ï¼Œé—®é¢˜ä¸åœ¨æ¢¯åº¦æ•°å­¦ï¼Œè€Œåœ¨è®­ç»ƒèŒƒå¼/æ¶æ„æ·±åº¦ã€‚")

    return results


# ==========================================================================
#  CLI å…¥å£
# ==========================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Anla Wirtinger æœ‰é™å·®åˆ†æ¢¯åº¦éªŒè¯"
    )
    parser.add_argument(
        "--layer", nargs="+", default=["all"],
        help="è¦æµ‹è¯•çš„å±‚ (é»˜è®¤: all). å¯é€‰: MagPhase, Holographic, PhaseTwist, "
             "RMSNorm, Linear, Rotary, Transformer, Softmax, Param"
    )
    parser.add_argument("--eps", type=float, default=1e-5, help="æœ‰é™å·®åˆ†æ­¥é•¿")
    parser.add_argument("--n-samples", type=int, default=20, help="æ¯å±‚é‡‡æ ·ç‚¹æ•°")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")

    args = parser.parse_args()

    layers = args.layer if args.layer != ["all"] else None
    run_all_tests(eps=args.eps, n_samples=args.n_samples, seed=args.seed, layers=layers)


if __name__ == "__main__":
    main()
